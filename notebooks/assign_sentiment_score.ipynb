{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install requests\n",
    "#!pip install textblob\n",
    "#!pip install vaderSentiment\n",
    "#!pip install tweet-preprocessor\n",
    "#!pip install emoji\n",
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "import emoji\n",
    "import preprocessor as p\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import ast\n",
    "AFINN_PATH = '../sentiment/AFINN-111.txt'\n",
    "HEDONO_PATH = \"../sentiment/Data_Set_S1.txt\"\n",
    "TEXT_FIELD = 'pure_text'\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n",
    "str_weather_terms = '''aerovane air airstream altocumulus altostratus anemometer anemometers anticyclone anticyclones \\\n",
    "arctic arid aridity atmosphere atmospheric autumn autumnal balmy baroclinic barometer barometers \\\n",
    "barometric blizzard blizzards blustering blustery blustery breeze breezes breezy brisk calm \\\n",
    "celsius chill chilled chillier chilliest chilly chinook cirrocumulus cirrostratus cirrus climate climates \\\n",
    "cloud cloudburst cloudbursts cloudier cloudiest clouds cloudy cold colder coldest condensation \\\n",
    "contrail contrails cool cooled cooling cools cumulonimbus cumulus cyclone cyclones damp damp \\\n",
    "damper damper dampest dampest degree degrees deluge dew dews dewy doppler downburst \\\n",
    "downbursts downdraft downdrafts downpour downpours dried drier dries driest drizzle drizzled \\\n",
    "drizzles drizzly drought droughts dry dryline fall farenheit flood flooded flooding floods flurries \\\n",
    "flurry fog fogbow fogbows fogged fogging foggy fogs forecast forecasted forecasting forecasts freeze \\\n",
    "freezes freezing frigid frost frostier frostiest frosts frosty froze frozen gale gales galoshes gust \\\n",
    "gusting gusts gusty haboob haboobs hail hailed hailing hails haze hazes hazy heat heated heating \\\n",
    "heats hoarfrost hot hotter hottest humid humidity hurricane hurricanes ice iced ices icing icy \\\n",
    "inclement landspout landspouts lightning lightnings macroburst macrobursts maelstrom mercury \\\n",
    "meteorologic meteorologist meteorologists meteorology microburst microbursts microclimate \\\n",
    "microclimates millibar millibars mist misted mists misty moist moisture monsoon monsoons \\\n",
    "mugginess muggy nexrad nippy NOAA nor’easter nor’easters noreaster noreasters overcast ozone \\\n",
    "parched parching pollen precipitate precipitated precipitates precipitating precipitation psychrometer \\\n",
    "radar rain rainboots rainbow rainbows raincoat raincoats rained rainfall rainier rainiest \\\n",
    "raining rains rainy sandstorm sandstorms scorcher scorching searing shower showering showers \\\n",
    "skiff sleet slicker slickers slush slushy smog smoggier smoggiest smoggy snow snowed snowier \\\n",
    "snowiest snowing snowmageddon snowpocalypse snows snowy spring sprinkle sprinkles sprinkling \\\n",
    "squall squalls squally storm stormed stormier stormiest storming storms stormy stratocumulus \\\n",
    "stratus subtropical summer summery sun sunnier sunniest sunny temperate temperature tempest \\\n",
    "thaw thawed thawing thaws thermometer thunder thundered thundering thunders thunderstorm \\\n",
    "thunderstorms tornadic tornado tornadoes tropical troposphere tsunami turbulent twister twisters \\\n",
    "typhoon typhoons umbrella umbrellas vane warm warmed warming warms warmth waterspout \\\n",
    "waterspouts weather wet wetter wettest wind windchill windchills windier windiest windspeed \\\n",
    "windy winter wintery wintry'''\n",
    "LST_WEATHER_TERMS = str_weather_terms.split(' ')\n",
    "DICT_WEATHER_TERMS = {LST_WEATHER_TERMS[i]: 1 for i in range(len(LST_WEATHER_TERMS))}\n",
    "\n",
    "def CheckWeatherTerm(text):\n",
    "    '''\n",
    "    Return 1 or 0 for whether input contains any weather term\n",
    "    '''\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for w in words:\n",
    "        if w in DICT_WEATHER_TERMS:\n",
    "            return 1\n",
    "    return 0\n",
    "#https://github.com/s/preprocessor/issues/50\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException\n",
    "\n",
    "def keepemoji_clean(text):\n",
    "    '''\n",
    "    clean text with tweet-preprocessor\n",
    "    '''\n",
    "    text = emoji.demojize(text)\n",
    "    import signal\n",
    "    signal.signal(signal.SIGALRM, timeout_handler)\n",
    "    signal.alarm(2)\n",
    "    try:\n",
    "        r = p.clean(text)\n",
    "    except TimeoutException:\n",
    "        print(f\"Could not handle the {text}\")\n",
    "        r = text\n",
    "    else:\n",
    "        signal.alarm(0)\n",
    "\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def afinn_sentiment(text,afinn,pattern_split):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence. \n",
    "    \"\"\"\n",
    "    words = pattern_split.split(text.lower())\n",
    "    sentiments = list(map(lambda word: afinn.get(word, 0), words))\n",
    "    leng = len(sentiments)\n",
    "    if leng > 0:\n",
    "        # How should you weight the individual word sentiments? \n",
    "        # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)\n",
    "        sentiment = float(sum(list(sentiments)))/math.sqrt(len(list(sentiments)))\n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment\n",
    "\n",
    "def add_afinn(df):\n",
    "    filenameAFINN = AFINN_PATH\n",
    "    afinn = dict(map(lambda ws: (ws[0], int(ws[1])), [ \n",
    "            ws.strip().split('\\t') for ws in open(filenameAFINN) ]))\n",
    "    pattern_split = re.compile(r\"\\W+\")\n",
    "    df['afinn'] = df[TEXT_FIELD].map(lambda x:afinn_sentiment(x,afinn,pattern_split))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scores(filename):\n",
    "    \"\"\"Takes a file from the Dodd research paper and returns a dict of\n",
    "    wordscores. Note this function is tailored to the file provided\n",
    "    by the Dodd paper. For other sets of word scores, a dict can be\n",
    "    passed directly to HMeter.\"\"\"\n",
    "    doddfile = csv.reader(open(filename, \"r\"), delimiter='\\t')\n",
    "    for x in range(4):  # strip header info\n",
    "        next(doddfile)\n",
    "    return {row[0]: float(row[2]) for row in doddfile}\n",
    "\n",
    "class HMeter(object):\n",
    "    \"\"\"HMeter is the main class to prepare a text sample for scores. It\n",
    "    expects a list of individual words, such as those provided by \n",
    "    nltk.word_tokenize, as wordlist. It expects a dict of words as k and\n",
    "    floating point wordscores as v for wordscores. deltah allows us to \n",
    "    filter out the most neutral words as stop words.\"\"\"\n",
    "    def __init__(self, wordlist, wordscores, deltah=0.0):\n",
    "        self.wordlist = wordlist\n",
    "        self.wordscores = wordscores\n",
    "        self.deltah = deltah\n",
    "    _deltah = None\n",
    "    @property\n",
    "    def deltah(self):\n",
    "        \"\"\"Deltah determines stop words. The higher deltah the more neutral \n",
    "        words are are discarded from the matchlist.\"\"\"\n",
    "        return self._deltah\n",
    "    @deltah.setter\n",
    "    def deltah(self, deltah):\n",
    "        \"\"\"Each time deltah is set we need to regenerate the matchlist.\"\"\"\n",
    "        self._deltah = deltah\n",
    "        # TODO Should probably raise a range error if deltah is nonsensical\n",
    "        # first we take every word that matches labMT 1.0\n",
    "        labmtmatches = (word for word in self.wordlist\n",
    "                        if word in self.wordscores)\n",
    "        # then we strip out stop words as described by Dodd paper\n",
    "        self.matchlist = []\n",
    "        for word in labmtmatches:\n",
    "            score = self.wordscores[word]\n",
    "            if score >= 5.0 + self.deltah or score <= 5.0 - self.deltah:\n",
    "                self.matchlist.append(word)\n",
    "    def fractional_abundance(self, word):\n",
    "        \"\"\"Takes a word and return its fractional abundance within\n",
    "        self.matchlist\"\"\"\n",
    "        frac_abund = self.matchlist.count(word) / len(self.matchlist)\n",
    "        return frac_abund\n",
    "    def word_shift(self, comp):\n",
    "        \"\"\"Produces data necessary to create a word shift graph. Returns a list \n",
    "        of tuples that contain each word's contribution to happiness score shift \n",
    "        between two samples. So for example, assigned to a variable 'output_data'\n",
    "        output_data[n] represents the data for one word where:\n",
    "            \n",
    "        output_data[n][0] the word\n",
    "        output_data[n][1] the proportional contribution the word gives to overall\n",
    "                          word shift\n",
    "        output_data[n][2] The relative abundance of word between the two samples\n",
    "        output_data[n][3] The word's happiness relative to the refernce sample\n",
    "        \n",
    "        Using this data, we can construct word shift graphs as described here:\n",
    "        http://www.hedonometer.org/shifts.html\"\"\"\n",
    "        # initialize variables for potentially large loop.\n",
    "        # create our comparison object. self is the reference object.\n",
    "        tcomp = HMeter(comp, self.deltah)\n",
    "        # we want a list of all potential words, but only need each word once.\n",
    "        word_shift_list = set(tcomp.matchlist + self.matchlist)\n",
    "        output_data = []\n",
    "        ref_happiness_score = self.happiness_score()\n",
    "        comp_happiness_score = tcomp.happiness_score()\n",
    "        happy_diff = comp_happiness_score - ref_happiness_score\n",
    "        for word in word_shift_list:\n",
    "            abundance = (tcomp.fractional_abundance(word) -\n",
    "                         self.fractional_abundance(word))\n",
    "            happiness_shift = self.wordscores[word] - ref_happiness_score\n",
    "            paper_score = (happiness_shift * abundance * 100) / happy_diff\n",
    "            output_data.append((word, paper_score, abundance, happiness_shift))\n",
    "        # sort words by absolute value of individual word shift\n",
    "        output_data.sort(key=lambda word: abs(word[1]))\n",
    "        return output_data\n",
    "    def happiness_score(self):\n",
    "        \"\"\"Takes a list made up of individual words and returns the happiness\n",
    "        score.\"\"\"\n",
    "        happysum = 0\n",
    "        count = len(self.matchlist)\n",
    "        for word in self.matchlist:\n",
    "            happysum += self.wordscores[word]\n",
    "        if count != 0:  # divide by zero errors are sad.\n",
    "            return happysum / count\n",
    "        else:\n",
    "            pass  # empty lists have no score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def hmeter_sentiment(text,pattern_split,scores):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence. \n",
    "    \"\"\"\n",
    "    words = pattern_split.split(text.lower())\n",
    "    h = HMeter(words,scores)\n",
    "    return h.happiness_score()\n",
    "\n",
    "def add_hedono(df):\n",
    "    scores = load_scores(HEDONO_PATH)\n",
    "    pattern_split = re.compile(r\"\\W+\")\n",
    "    df['hedono'] = df[TEXT_FIELD].map(lambda x:hmeter_sentiment(x,pattern_split,scores))\n",
    "    return df\n",
    "\n",
    "def add_vader(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['vader'] = df[TEXT_FIELD].map(lambda x:analyzer.polarity_scores(x)['compound'])\n",
    "    return df\n",
    "def add_all_sentiment(df):\n",
    "    '''\n",
    "    calculate sentiment scores for field TEXT_FIELD\n",
    "    '''\n",
    "    df = add_afinn(df)\n",
    "    df = add_hedono(df)\n",
    "    df = add_vader(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "tweet_df = pd.read_csv(\"../../../data/tweets/csv/usa_tweets_2012_chunk_9.csv\",lineterminator='\\n',dtype={'id':str,'tweet_created_at':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = '/gpfs/data1/oshangp/liuz/sesync'\n",
    "READ_PATH = PROJECT_ROOT+'/data/tweets/csv'\n",
    "SAVE_PATH = PROJECT_ROOT+'/data/processed/sen'\n",
    "donefiles = os.listdir(SAVE_PATH)\n",
    "files = [os.path.join(READ_PATH, f) for f in os.listdir(READ_PATH) if f.endswith('.csv') and f not in donefiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs/data1/oshangp/liuz/sesync/data/tweets/csv/usa_tweets_2014_a_chunk_12.csv',\n",
       " '/gpfs/data1/oshangp/liuz/sesync/data/tweets/csv/usa_tweets_2014_b_chunk_8.csv',\n",
       " '/gpfs/data1/oshangp/liuz/sesync/data/tweets/csv/usa_tweets_2014_c_chunk_11.csv',\n",
       " '/gpfs/data1/oshangp/liuz/sesync/data/tweets/csv/usa_tweets_2014_a_chunk_21.csv',\n",
       " '/gpfs/data1/oshangp/liuz/sesync/data/tweets/csv/usa_tweets_2012_chunk_9.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 29)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982358            @_CathyVazquez @TheBucktList ay dios mio😶\n",
       "657072    As usual, tinawanan lang ako. And may sira na ...\n",
       "240282    Ilang years na ba akong di nakapagbowling........\n",
       "467925         Magbabad sa bath ng 10 min then higa na. 👄👅👌\n",
       "507199                             DJ ang in the hizz house\n",
       "5878                   Sobrang chiks ng mga clippers dancer\n",
       "155161    Sumigaw ako ng \"natatae ako\" nakalimutan kong ...\n",
       "113428    Marami naman talaga iba jan e. Ikaw nga lang t...\n",
       "902925    “@ateIrms: @kkkath000: Matteo at Sarah palalim...\n",
       "485481    @Blessed7th @abbygaiil__ @just1n1nt1me22 ay ay...\n",
       "957696                                         ay que tuani\n",
       "551702                                   @Succ_Da_Lang wyo?\n",
       "136755                               @TunityTV Gotcha. TKS!\n",
       "954378    @kaiceVK14 kaya mag-UD ka na! Chos! Hahaha gal...\n",
       "712509    @ayeemndz kayo nalang ni tatay ulit nay hahaha...\n",
       "18845     Kahit madaming tao gusto ko parin pumunta bsta...\n",
       "722926    @Coriyssa sarap naman ng buhay bum! Twitter tw...\n",
       "282657    @friendswithnico 히히히히히히 Ngani. 😂😂 May ireto ak...\n",
       "720877    Buti nlng my clean up app ako kung hindi Wala!...\n",
       "12395      @jamtolibas @hppypzz ganda ng 2015 ni kamil 😂😂😂😂\n",
       "Name: pure_text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tweet_df[tweet_df['fastText_lang']=='tl'].sample(20).pure_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(df):\n",
    "    df = df[df['pure_text'].notna()]\n",
    "    df['tweet_created_at_int'] = df['tweet_created_at'].apply(lambda x: json.loads(x.replace(\"'\", '\"'))['$date'])\n",
    "    df = df[['id','pure_text','tweet_created_at_int','fastText_lang','lat','lon']]\n",
    "    df['clean_text'] = df['pure_text'].map(lambda x:keepemoji_clean(x))\n",
    "    df['weather_term'] = df['clean_text'].map(lambda x:CheckWeatherTerm(x))\n",
    "    df = add_all_sentiment(df)\n",
    "    df = df.drop(columns=['pure_text','clean_text'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_140882/735504801.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "tweet_df.iloc[i:i+5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    " txt = tweet_df.iloc[762192:762193]['pure_text'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = emoji.demojize(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '“@CalJDaGeneral45: @MHBEATS http://t.co/3P5Xpe4T”!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“@CalJDaGeneral45: @MHBEATS http://t.co/3P5Xpe4T”!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_159372/1976141321.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/api.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(tweet_string)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mPreprocessor\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcleaned_tweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLEAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_tweet_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/preprocess.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(self, tweet_string, repl)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrepl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLEAN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mtweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_to_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_to_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/preprocess.py\u001b[0m in \u001b[0;36mpreprocess_urls\u001b[0;34m(self, tweet_string, repl)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPatterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURL_PATTERN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p.clean(txt           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>fastText_conf</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>user_created_at</th>\n",
       "      <th>pure_text</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>rt_text</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hedono</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23576</th>\n",
       "      <td>Tue Apr 29 20:28:40 +0000 2014</td>\n",
       "      <td>461240673715224600</td>\n",
       "      <td>461240673715224576</td>\n",
       "      <td>Google+\\n\\nhttps://t.co/yZqsJwzaB2\\n\\n...........</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>{'$date': 1398803320000}</td>\n",
       "      <td>{'$date': 1352905527000}</td>\n",
       "      <td>Google+\\n\\nhttps://t.co/yZqsJwzaB2\\n\\n...........</td>\n",
       "      <td>42.949427</td>\n",
       "      <td>-87.894438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.456667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           created_at                  id              id_str  \\\n",
       "23576  Tue Apr 29 20:28:40 +0000 2014  461240673715224600  461240673715224576   \n",
       "\n",
       "                                                    text  \\\n",
       "23576  Google+\\n\\nhttps://t.co/yZqsJwzaB2\\n\\n...........   \n",
       "\n",
       "                                                  source  truncated  \\\n",
       "23576  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "\n",
       "       in_reply_to_status_id  in_reply_to_status_id_str  in_reply_to_user_id  \\\n",
       "23576                    NaN                        NaN                  NaN   \n",
       "\n",
       "       in_reply_to_user_id_str  ... fastText_conf          tweet_created_at  \\\n",
       "23576                      NaN  ...          0.86  {'$date': 1398803320000}   \n",
       "\n",
       "                user_created_at  \\\n",
       "23576  {'$date': 1352905527000}   \n",
       "\n",
       "                                               pure_text        lat  \\\n",
       "23576  Google+\\n\\nhttps://t.co/yZqsJwzaB2\\n\\n...........  42.949427   \n",
       "\n",
       "             lon rt_text  afinn    hedono vader  \n",
       "23576 -87.894438     NaN    0.0  5.456667   0.0  \n",
       "\n",
       "[1 rows x 33 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_all_sentiment(tweet_df.iloc[23576:23577])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762192    “@CalJDaGeneral45: @MHBEATS http://t.co/3P5Xpe...\n",
       "Name: pure_text, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_df.iloc[762192:762193].pure_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not handle the “@CalJDaGeneral45: @MHBEATS http://t.co/3P5Xpe4T”!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_159372/970166312.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_159372/1050656441.py\u001b[0m in \u001b[0;36msentiment\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4158\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \"\"\"\n\u001b[0;32m-> 4160\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4161\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_159372/1050656441.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_159372/469505313.py\u001b[0m in \u001b[0;36mCheckWeatherTerm\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0minput\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0many\u001b[0m \u001b[0mweather\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     '''\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDICT_WEATHER_TERMS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     ]\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     return [\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     ]\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/destructive.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_parentheses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTING_QUOTES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubstitution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubstitution\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPUNCTUATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentiment(tweet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750000\n",
      "755000\n",
      "Could not handle the “@CalJDaGeneral45: @MHBEATS http://t.co/3P5Xpe4T”!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "760000\n",
      "765000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_159372/4258859826.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m750000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_159372/1050656441.py\u001b[0m in \u001b[0;36msentiment\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4158\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \"\"\"\n\u001b[0;32m-> 4160\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4161\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_159372/1050656441.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_159372/469505313.py\u001b[0m in \u001b[0;36mCheckWeatherTerm\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0minput\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0many\u001b[0m \u001b[0mweather\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     '''\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDICT_WEATHER_TERMS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \"\"\"\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     return [\n\u001b[1;32m    132\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m    107\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1326\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m         \"\"\"\n\u001b[0;32m-> 1328\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1319\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \"\"\"\n\u001b[1;32m   1358\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(750000,len(tweet_df),5000):\n",
    "    sentiment(tweet_df.iloc[i:i+5000])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_140882/3516727029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# For example, a loop or a function call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msentiment_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#sentiment_df['tweet_created_at'].apply(lambda x: json.loads(x.replace(\"'\", '\"'))['$date'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#sentiment_df['tweet_created_at'].apply(lambda x: ast.literal_eval(x)['$date'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_140882/1050656441.py\u001b[0m in \u001b[0;36msentiment\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'$date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   4158\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4159\u001b[0m         \"\"\"\n\u001b[0;32m-> 4160\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4161\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[1;32m   4162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"map\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_map_values\u001b[0;34m(self, mapper, na_action)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# mapper is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_140882/1050656441.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_created_at'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'$date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tweet_created_at_int'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fastText_lang'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pure_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mkeepemoji_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weather_term'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clean_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mCheckWeatherTerm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_all_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_140882/3871008280.py\u001b[0m in \u001b[0;36mkeepemoji_clean\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     58\u001b[0m     '''\n\u001b[1;32m     59\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memoji\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemojize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/api.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(tweet_string)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mPreprocessor\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mcleaned_tweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLEAN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_tweet_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/preprocess.py\u001b[0m in \u001b[0;36mclean\u001b[0;34m(self, tweet_string, repl)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrepl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLEAN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                 \u001b[0mtweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_to_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mtweet_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod_to_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/preprocessor/preprocess.py\u001b[0m in \u001b[0;36mpreprocess_urls\u001b[0;34m(self, tweet_string, repl)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPatterns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mURL_PATTERN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_hashtags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Your code here\n",
    "# For example, a loop or a function call\n",
    "sentiment_df = sentiment(tweet_df)\n",
    "#sentiment_df['tweet_created_at'].apply(lambda x: json.loads(x.replace(\"'\", '\"'))['$date'])\n",
    "#sentiment_df['tweet_created_at'].apply(lambda x: ast.literal_eval(x)['$date'])\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>tweet_created_at_int</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>weather_term</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hedono</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>550441276752293888</td>\n",
       "      <td>1420070401000</td>\n",
       "      <td>39.905443</td>\n",
       "      <td>-86.077531</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.336306</td>\n",
       "      <td>4.924000</td>\n",
       "      <td>-0.2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>550441280661360641</td>\n",
       "      <td>1420070402000</td>\n",
       "      <td>33.987111</td>\n",
       "      <td>-83.984996</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>0.7672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>550441280892067841</td>\n",
       "      <td>1420070402000</td>\n",
       "      <td>36.318364</td>\n",
       "      <td>-115.213250</td>\n",
       "      <td>0</td>\n",
       "      <td>0.904534</td>\n",
       "      <td>5.530000</td>\n",
       "      <td>0.3818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550441280858112000</td>\n",
       "      <td>1420070402000</td>\n",
       "      <td>34.374670</td>\n",
       "      <td>-118.592765</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.705714</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>550441280853901312</td>\n",
       "      <td>1420070402000</td>\n",
       "      <td>31.592575</td>\n",
       "      <td>-102.885392</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.124000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>552216771814772736</td>\n",
       "      <td>1420493712000</td>\n",
       "      <td>43.410781</td>\n",
       "      <td>-84.609748</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.710000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>552216771667587072</td>\n",
       "      <td>1420493712000</td>\n",
       "      <td>29.714756</td>\n",
       "      <td>-95.380429</td>\n",
       "      <td>0</td>\n",
       "      <td>1.459601</td>\n",
       "      <td>5.724286</td>\n",
       "      <td>0.7783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>552216775639969792</td>\n",
       "      <td>1420493713000</td>\n",
       "      <td>39.553933</td>\n",
       "      <td>-76.388266</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>552216775681515520</td>\n",
       "      <td>1420493713000</td>\n",
       "      <td>30.292801</td>\n",
       "      <td>-97.744604</td>\n",
       "      <td>0</td>\n",
       "      <td>0.267261</td>\n",
       "      <td>5.868333</td>\n",
       "      <td>0.1027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>552216775736053760</td>\n",
       "      <td>1420493713000</td>\n",
       "      <td>34.134664</td>\n",
       "      <td>-117.883252</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.896000</td>\n",
       "      <td>0.8271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999222 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id_str  tweet_created_at_int        lat         lon  \\\n",
       "0       550441276752293888         1420070401000  39.905443  -86.077531   \n",
       "1       550441280661360641         1420070402000  33.987111  -83.984996   \n",
       "2       550441280892067841         1420070402000  36.318364 -115.213250   \n",
       "3       550441280858112000         1420070402000  34.374670 -118.592765   \n",
       "4       550441280853901312         1420070402000  31.592575 -102.885392   \n",
       "...                    ...                   ...        ...         ...   \n",
       "999995  552216771814772736         1420493712000  43.410781  -84.609748   \n",
       "999996  552216771667587072         1420493712000  29.714756  -95.380429   \n",
       "999997  552216775639969792         1420493713000  39.553933  -76.388266   \n",
       "999998  552216775681515520         1420493713000  30.292801  -97.744604   \n",
       "999999  552216775736053760         1420493713000  34.134664 -117.883252   \n",
       "\n",
       "        weather_term     afinn    hedono   vader  \n",
       "0                  0 -1.336306  4.924000 -0.2023  \n",
       "1                  0  0.500000  6.280000  0.7672  \n",
       "2                  0  0.904534  5.530000  0.3818  \n",
       "3                  1  0.000000  5.705714  0.0000  \n",
       "4                  0  0.000000  5.124000  0.0000  \n",
       "...              ...       ...       ...     ...  \n",
       "999995             0  0.000000  5.710000  0.0000  \n",
       "999996             0  1.459601  5.724286  0.7783  \n",
       "999997             0  0.000000       NaN  0.0000  \n",
       "999998             0  0.267261  5.868333  0.1027  \n",
       "999999             0  0.000000  5.896000  0.8271  \n",
       "\n",
       "[999222 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_str</th>\n",
       "      <th>tweet_created_at</th>\n",
       "      <th>weather_term</th>\n",
       "      <th>afinn</th>\n",
       "      <th>hedono</th>\n",
       "      <th>vader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>550441280858112000</td>\n",
       "      <td>{'$date': 1420070402000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.705714</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>550441280929406977</td>\n",
       "      <td>{'$date': 1420070402000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>5.660000</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>550441330988421122</td>\n",
       "      <td>{'$date': 1420070414000}</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.224745</td>\n",
       "      <td>5.136667</td>\n",
       "      <td>-0.6447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>550441356502376448</td>\n",
       "      <td>{'$date': 1420070420000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.502667</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>550441410852179968</td>\n",
       "      <td>{'$date': 1420070433000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.320000</td>\n",
       "      <td>-0.3353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999863</th>\n",
       "      <td>552216528259915776</td>\n",
       "      <td>{'$date': 1420493654000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229416</td>\n",
       "      <td>5.570000</td>\n",
       "      <td>0.5707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999879</th>\n",
       "      <td>552216565920579584</td>\n",
       "      <td>{'$date': 1420493663000}</td>\n",
       "      <td>1</td>\n",
       "      <td>1.066004</td>\n",
       "      <td>5.517778</td>\n",
       "      <td>0.8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999947</th>\n",
       "      <td>552216683730173952</td>\n",
       "      <td>{'$date': 1420493691000}</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.242536</td>\n",
       "      <td>5.682000</td>\n",
       "      <td>0.4567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999959</th>\n",
       "      <td>552216708845666305</td>\n",
       "      <td>{'$date': 1420493697000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.3182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999971</th>\n",
       "      <td>552216733935611904</td>\n",
       "      <td>{'$date': 1420493703000}</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.241667</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27956 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id_str          tweet_created_at  weather_term     afinn  \\\n",
       "3       550441280858112000  {'$date': 1420070402000}             1  0.000000   \n",
       "6       550441280929406977  {'$date': 1420070402000}             1  0.377964   \n",
       "51      550441330988421122  {'$date': 1420070414000}             1 -1.224745   \n",
       "72      550441356502376448  {'$date': 1420070420000}             1  0.000000   \n",
       "121     550441410852179968  {'$date': 1420070433000}             1  0.000000   \n",
       "...                    ...                       ...           ...       ...   \n",
       "999863  552216528259915776  {'$date': 1420493654000}             1  0.229416   \n",
       "999879  552216565920579584  {'$date': 1420493663000}             1  1.066004   \n",
       "999947  552216683730173952  {'$date': 1420493691000}             1 -0.242536   \n",
       "999959  552216708845666305  {'$date': 1420493697000}             1  0.577350   \n",
       "999971  552216733935611904  {'$date': 1420493703000}             1  0.000000   \n",
       "\n",
       "          hedono   vader  \n",
       "3       5.705714  0.0000  \n",
       "6       5.660000  0.3182  \n",
       "51      5.136667 -0.6447  \n",
       "72      5.502667  0.0000  \n",
       "121     5.320000 -0.3353  \n",
       "...          ...     ...  \n",
       "999863  5.570000  0.5707  \n",
       "999879  5.517778  0.8074  \n",
       "999947  5.682000  0.4567  \n",
       "999959  7.200000  0.3182  \n",
       "999971  5.241667 -0.2500  \n",
       "\n",
       "[27956 rows x 6 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df.query('weather_term > 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_set = set(tweet_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path containing the CSV files\n",
    "folder_path = \"../../../data/tweets/csv/*.csv\"\n",
    "\n",
    "# Initialize a variable to keep track of the total row count\n",
    "total_rows = 0\n",
    "\n",
    "# Use glob to get a list of CSV file paths in the folder\n",
    "csv_files = glob.glob(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../../data/tweets/csv/usa_tweets_2014_a_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_38.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_31.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_29.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_44.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_34.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2010_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_42.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_37.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_27.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_26.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_31.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_29.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_36.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2010_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_26.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_27.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2010_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_40.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_25.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_30.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_27.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_39.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_29.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_35.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_32.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_30.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2019.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_41.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2020.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_25.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_32.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_31.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_34.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_28.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_34.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_28.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_32.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_35.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2010_chunk_3.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_25.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2016.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_25.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_26.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_30.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_29.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_2.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_5.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_7.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_27.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2017.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_27.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_43.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_23.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_33.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_28.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_33.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_12.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_26.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_8.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2018.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_22.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2009_chunk_1.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2010_chunk_4.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_15.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_19.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_18.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_14.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_13.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_16.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_20.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_24.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_25.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_b_chunk_33.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2013_chunk_6.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_35.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_28.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_11.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2012_chunk_9.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_c_chunk_21.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_10.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2014_a_chunk_26.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2015_chunk_17.csv',\n",
       " '../../../data/tweets/csv/usa_tweets_2011_chunk_13.csv']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for csv_file in csv_files:\n",
    "#     tweet_df = pd.read_csv(csv_file,nrows=6)\n",
    "#     d1 = tmp_set - set(tweet_df.columns.tolist())\n",
    "#     d2 = set(tweet_df.columns.tolist()) - tmp_set\n",
    "#     if d1:\n",
    "#         print(csv_file,d1)\n",
    "#     if d2:\n",
    "#         print(csv_file,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweet_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (18,19,20) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (2,26,27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (14,26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (25,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,30,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,28,29,31) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32,35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (35) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,28,29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (19,29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (10,26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (21,22,23) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (9) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (29) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,29,30,31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (18,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (24) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (13,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (31,32) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n",
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/multiprocessing/pool.py:44: DtypeWarning: Columns (0,1,3,4,5,10,11,12,14,17,18,19,20,21,22,31,34,35,36,37,38,39,40,41,42,43,44,45,46,47,49,50,51,52,54,55,56,57) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  return list(map(*args))\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "def count_lines_in_csv(file_path):\n",
    "    try:\n",
    "        # with open(file_path, \"r\",newline='') as file:\n",
    "        #     csv_reader = csv.reader(file)\n",
    "        #     # Count the rows in the current CSV file and add to the total\n",
    "        #     len_lines = sum(1 for row in csv_reader)\n",
    "        #     #print(file_path)\n",
    "        len_lines = pd.read_csv(file_path,lineterminator='\\n').shape[0]\n",
    "        return len_lines\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return 0  # Return 0 lines in case of an error\n",
    "def count_language_in_csv(file_path):\n",
    "    try:\n",
    "        # with open(file_path, \"r\",newline='') as file:\n",
    "        #     csv_reader = csv.reader(file)\n",
    "        #     # Count the rows in the current CSV file and add to the total\n",
    "        #     len_lines = sum(1 for row in csv_reader)\n",
    "        #     #print(file_path)\n",
    "        df = pd.read_csv(file_path,lineterminator='\\n') \n",
    "        len_lines = df.shape[0]\n",
    "        if 'fastText_lang' in df.columns:\n",
    "            col_language = 'fastText_lang'\n",
    "        else:\n",
    "            print(f'no lang col in {file_path}')\n",
    "            return 0,0\n",
    "        lang_en = df[df[col_language]=='en'].shape[0]\n",
    "        return len_lines,lang_en\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return 0,0  # Return 0 lines in case of an error\n",
    "\n",
    "# Create a multiprocessing pool\n",
    "pool = multiprocessing.Pool(20)\n",
    "\n",
    "# Map the function to count lines to the file paths\n",
    "#line_counts = pool.map(count_lines_in_csv, csv_files)\n",
    "line_counts = pool.map(count_language_in_csv, csv_files)\n",
    "# Close the pool and wait for the processes to finish\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "# Calculate the total lines by summing up individual counts\n",
    "#total_lines = sum(line_counts)\n",
    "#print(f\"Total lines in all CSV files: {total_lines}\")\n",
    "\n",
    "# Separate and sum the elements\n",
    "sum_a = sum(a for a, b in line_counts)\n",
    "sum_b = sum(b for a, b in line_counts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231310037"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206192313"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8914110069508138"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of english detect by \n",
    "sum_b/sum_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 367033,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 255910,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 490916,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 488941,\n",
       " 475565,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 3346163,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 210899,\n",
       " 277558,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 4583363,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 983263,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 7003754,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 5272161,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 47016,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 507495,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000,\n",
       " 1000000]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "../../../data/tweets/csv/usa_tweets_2011_chunk_2.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/data1/oshangp/liuz/gs7_gpd/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3441: DtypeWarning: Columns (14,26) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../../../data/tweets/csv/usa_tweets_2011_chunk_2.csv',lineterminator='\\n').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
