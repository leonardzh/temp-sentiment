{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Portal\n",
    "<p>\n",
    "    \n",
    "This notebook is for producing and manipulating social media dataset we will use in the Text Data Climate Shock project. \n",
    "\n",
    "In Part One, a social media microblog with geo-tags will be loaded, cleaned, and translated if it contains text in language other than English. \n",
    "\n",
    "In Part Two, six kinds of sentiment scores for each tweet will be calculated and [combined for a weighted sum in Pandas DataFrame].\n",
    "</p>\n",
    "\n",
    "<h5>\n",
    "supported input data: Twitter, [Sina Weibo]\n",
    "\n",
    "</h5>\n",
    "<h5>\n",
    "supported translator: Google Translate API, Microsoft Azure Translator Text API, [offline dictionary]\n",
    "</h5>\n",
    "<h5>\n",
    "supported sentiment measure: AFINN,Textblob, Hedonometer, VADER, SentiWordNet, WKWSCI Sentiment Lexicon, [LIWC 2015]\n",
    "\n",
    "</h5>\n",
    "<p>in []: Under development / Pending to use\n",
    "    </p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries needed\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import requests, uuid\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import math\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "import preprocessor as p\n",
    "import emoji\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from googletrans import Translator\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install/download if needed\n",
    "#!pip install pymysql --user\n",
    "#!pip3 install --user tweet-preprocessor\n",
    "#!pip3 install --user emoji\n",
    "#!pip3 install --user googletrans\n",
    "#!pip3 install -U --user textblob\n",
    "#!pip3 install --user vaderSentiment\n",
    "\n",
    "#nltk.download('sentiwordnet')\n",
    "#!python3 -m textblob.download_corpora\n",
    "#!wget http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE GLOBAL VARIABLES\n",
    "\n",
    "#Paths\n",
    "READ_PATH = 'archive_india/'\n",
    "SAVE_PATH = 'india_score/'\n",
    "\n",
    "AFINN_PATH = 'sentiment_lexicon/AFINN/AFINN-111.txt'\n",
    "HEDONO_PATH = \"sentiment_lexicon/Data_Set_S1.txt\"\n",
    "WKWSCI_PATH = 'sentiment_lexicon/WKWSCISenti.tab'\n",
    "#AZURE VARIABLE\n",
    "AZURE_KEY = '5f109b1515f642e3a05b8d80f24a9cdd'\n",
    "\n",
    "\n",
    "# field name that will calculate for sentiment\n",
    "TEXT_FIELD = 'en_text'\n",
    "\n",
    "#Define a function convert 2-digit language code to language name in English\n",
    "iso_639_choices = [('ab', 'Abkhaz'),('aa', 'Afar'),('af', 'Afrikaans'),('ak', 'Akan'),('sq', 'Albanian'),\n",
    "('am', 'Amharic'),('ar', 'Arabic'),('an', 'Aragonese'),('hy', 'Armenian'),('as', 'Assamese'),('av', 'Avaric'),\n",
    "('ae', 'Avestan'),('ay', 'Aymara'),('az', 'Azerbaijani'),('bm', 'Bambara'),('ba', 'Bashkir'),('eu', 'Basque'),\n",
    "('be', 'Belarusian'),('bn', 'Bengali'),('bh', 'Bihari'),('bi', 'Bislama'),('bs', 'Bosnian'),('br', 'Breton'),('bg', 'Bulgarian'),\n",
    "('my', 'Burmese'),('ca', 'Catalan; Valencian'),('ch', 'Chamorro'),('ce', 'Chechen'),('ny', 'Chichewa; Chewa; Nyanja'),\n",
    "('zh', 'Chinese'),('cv', 'Chuvash'),('kw', 'Cornish'),('co', 'Corsican'),('cr', 'Cree'),('hr', 'Croatian'),('cs', 'Czech'),\n",
    "('da', 'Danish'),('dv', 'Divehi; Maldivian;'),('nl', 'Dutch'),('dz', 'Dzongkha'),('en', 'English'),\n",
    "('eo', 'Esperanto'),('et', 'Estonian'),('ee', 'Ewe'),('fo', 'Faroese'),('fj', 'Fijian'),('fi', 'Finnish'),\n",
    "('fr', 'French'),('ff', 'Fula'),('gl', 'Galician'),('ka', 'Georgian'),('de', 'German'),\n",
    "('el', 'Greek, Modern'),('gn', 'Guaraní'),('gu', 'Gujarati'),('ht', 'Haitian'),('ha', 'Hausa'),\n",
    "('he', 'Hebrew (modern)'),('hz', 'Herero'),('hi', 'Hindi'),('ho', 'Hiri Motu'),('hu', 'Hungarian'),\n",
    "('ia', 'Interlingua'),('id', 'Indonesian'),('ie', 'Interlingue'),('ga', 'Irish'),('ig', 'Igbo'),\n",
    "('ik', 'Inupiaq'),('io', 'Ido'),('is', 'Icelandic'),('it', 'Italian'),('iu', 'Inuktitut'),\n",
    "('ja', 'Japanese'),('jv', 'Javanese'),('kl', 'Kalaallisut'),('kn', 'Kannada'),\n",
    "('kr', 'Kanuri'),('ks', 'Kashmiri'),('kk', 'Kazakh'),('km', 'Khmer'),('ki', 'Kikuyu, Gikuyu'),\n",
    "('rw', 'Kinyarwanda'),('ky', 'Kirghiz, Kyrgyz'),('kv', 'Komi'),('kg', 'Kongo'),('ko', 'Korean'),\n",
    "('ku', 'Kurdish'),('kj', 'Kwanyama, Kuanyama'),('la', 'Latin'),('lb', 'Luxembourgish'),('lg', 'Luganda'),\n",
    "('li', 'Limburgish'),('ln', 'Lingala'),('lo', 'Lao'),('lt', 'Lithuanian'),('lu', 'Luba-Katanga'),\n",
    "('lv', 'Latvian'),('gv', 'Manx'),('mk', 'Macedonian'),('mg', 'Malagasy'),('ms', 'Malay'),\n",
    "('ml', 'Malayalam'),('mt', 'Maltese'),('mi', 'Māori'),('mr', 'Marathi (Marāṭhī)'),('mh', 'Marshallese'),\n",
    "('mn', 'Mongolian'),('na', 'Nauru'),('nv', 'Navajo, Navaho'),('nb', 'Norwegian Bokmål'),('nd', 'North Ndebele'),\n",
    "('ne', 'Nepali'),('ng', 'Ndonga'),('nn', 'Norwegian Nynorsk'),('no', 'Norwegian'),('ii', 'Nuosu'),\n",
    "('nr', 'South Ndebele'),('oc', 'Occitan'),('oj', 'Ojibwe, Ojibwa'),('cu', 'Old Church Slavonic'),('om', 'Oromo'),\n",
    "('or', 'Oriya'),('os', 'Ossetian, Ossetic'),('pa', 'Panjabi, Punjabi'),('pi', 'Pāli'),('fa', 'Persian'),\n",
    "('pl', 'Polish'),('ps', 'Pashto, Pushto'),('pt', 'Portuguese'),('qu', 'Quechua'),('rm', 'Romansh'),\n",
    "('rn', 'Kirundi'),('ro', 'Romanian, Moldavan'),('ru', 'Russian'),('sa', 'Sanskrit (Saṁskṛta)'),('sc', 'Sardinian'),\n",
    "('sd', 'Sindhi'),('se', 'Northern Sami'),('sm', 'Samoan'),('sg', 'Sango'),('sr', 'Serbian'),\n",
    "('gd', 'Scottish Gaelic'),('sn', 'Shona'),('si', 'Sinhala, Sinhalese'),('sk', 'Slovak'),('sl', 'Slovene'),\n",
    "('so', 'Somali'),('st', 'Southern Sotho'),('es', 'Spanish; Castilian'),('su', 'Sundanese'),('sw', 'Swahili'),('ss', 'Swati'),\n",
    "('sv', 'Swedish'),('ta', 'Tamil'),('te', 'Telugu'),('tg', 'Tajik'),('th', 'Thai'),('ti', 'Tigrinya'),\n",
    "('bo', 'Tibetan'),('tk', 'Turkmen'),('tl', 'Tagalog'),('tn', 'Tswana'),('to', 'Tonga'),('tr', 'Turkish'),('ts', 'Tsonga'),\n",
    "('tt', 'Tatar'),('tw', 'Twi'),('ty', 'Tahitian'),('ug', 'Uighur, Uyghur'),('uk', 'Ukrainian'),('ur', 'Urdu'),\n",
    "('uz', 'Uzbek'),('ve', 'Venda'),('vi', 'Vietnamese'),('vo', 'Volapük'),('wa', 'Walloon'),('cy', 'Welsh'),('wo', 'Wolof'),\n",
    "('fy', 'Western Frisian'),('xh', 'Xhosa'),('yi', 'Yiddish'),('yo', 'Yoruba'),('za', 'Zhuang, Chuang'),('zu', 'Zulu'),]\n",
    "LANG_CODES = dict(iso_639_choices)\n",
    "LANG_CODES['und'] = 'undefined'\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#Dictionary of weather related terms\n",
    "str_weather_terms = '''aerovane air airstream altocumulus altostratus anemometer anemometers anticyclone anticyclones \\\n",
    "arctic arid aridity atmosphere atmospheric autumn autumnal balmy baroclinic barometer barometers \\\n",
    "barometric blizzard blizzards blustering blustery blustery breeze breezes breezy brisk calm \\\n",
    "celsius chill chilled chillier chilliest chilly chinook cirrocumulus cirrostratus cirrus climate climates \\\n",
    "cloud cloudburst cloudbursts cloudier cloudiest clouds cloudy cold colder coldest condensation \\\n",
    "contrail contrails cool cooled cooling cools cumulonimbus cumulus cyclone cyclones damp damp \\\n",
    "damper damper dampest dampest degree degrees deluge dew dews dewy doppler downburst \\\n",
    "downbursts downdraft downdrafts downpour downpours dried drier dries driest drizzle drizzled \\\n",
    "drizzles drizzly drought droughts dry dryline fall farenheit flood flooded flooding floods flurries \\\n",
    "flurry fog fogbow fogbows fogged fogging foggy fogs forecast forecasted forecasting forecasts freeze \\\n",
    "freezes freezing frigid frost frostier frostiest frosts frosty froze frozen gale gales galoshes gust \\\n",
    "gusting gusts gusty haboob haboobs hail hailed hailing hails haze hazes hazy heat heated heating \\\n",
    "heats hoarfrost hot hotter hottest humid humidity hurricane hurricanes ice iced ices icing icy \\\n",
    "inclement landspout landspouts lightning lightnings macroburst macrobursts maelstrom mercury \\\n",
    "meteorologic meteorologist meteorologists meteorology microburst microbursts microclimate \\\n",
    "microclimates millibar millibars mist misted mists misty moist moisture monsoon monsoons \\\n",
    "mugginess muggy nexrad nippy NOAA nor’easter nor’easters noreaster noreasters overcast ozone \\\n",
    "parched parching pollen precipitate precipitated precipitates precipitating precipitation psychrometer \\\n",
    "radar rain rainboots rainbow rainbows raincoat raincoats rained rainfall rainier rainiest \\\n",
    "raining rains rainy sandstorm sandstorms scorcher scorching searing shower showering showers \\\n",
    "skiff sleet slicker slickers slush slushy smog smoggier smoggiest smoggy snow snowed snowier \\\n",
    "snowiest snowing snowmageddon snowpocalypse snows snowy spring sprinkle sprinkles sprinkling \\\n",
    "squall squalls squally storm stormed stormier stormiest storming storms stormy stratocumulus \\\n",
    "stratus subtropical summer summery sun sunnier sunniest sunny temperate temperature tempest \\\n",
    "thaw thawed thawing thaws thermometer thunder thundered thundering thunders thunderstorm \\\n",
    "thunderstorms tornadic tornado tornadoes tropical troposphere tsunami turbulent twister twisters \\\n",
    "typhoon typhoons umbrella umbrellas vane warm warmed warming warms warmth waterspout \\\n",
    "waterspouts weather wet wetter wettest wind windchill windchills windier windiest windspeed \\\n",
    "windy winter wintery wintry'''\n",
    "LST_WEATHER_TERMS = str_weather_terms.split(' ')\n",
    "DICT_WEATHER_TERMS = {LST_WEATHER_TERMS[i]: 1 for i in range(len(LST_WEATHER_TERMS))}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Raw to English Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Media Pull\n",
    "### Sina Weibo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get database connection\n",
    "def getConnection():\n",
    "\n",
    "    # Connect to the database\n",
    "    connection = pymysql.connect(host='10.105.131.16',\n",
    "                                 user='zliu',\n",
    "                                 password='climateshock',\n",
    "                                 db='bj_weibo_shaohu',\n",
    "                                 charset='utf8mb4',\n",
    "                                 cursorclass=pymysql.cursors.DictCursor)\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pullWeiboSamples():\n",
    "    connection = getConnection()\n",
    "    try:\n",
    "\n",
    "        with connection.cursor() as cursor:\n",
    "            # Read a single record\n",
    "            sql = \"SELECT `text`,`created_at_int`,`latitude`,`longitude` FROM `travel_poi_users_weibodata_beijing` limit 10;\"\n",
    "            cursor.execute(sql)\n",
    "            result = cursor.fetchall()\n",
    "            print(result)\n",
    "    finally:\n",
    "        connection.close()\n",
    "#demo run to pull some records\n",
    "#pullWeiboSamples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter\n",
    "\n",
    "Alternative choice:https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/5QCCUU&version=1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Tweets if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tarfile\n",
    "# tar = tarfile.open(\"western_europe_2012.tar.gz\")\n",
    "# tar.extractall()\n",
    "# tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keepemoji_clean(text):\n",
    "    '''\n",
    "    clean text with tweet-preprocessor\n",
    "    '''\n",
    "    text = emoji.demojize(text)\n",
    "    return p.clean(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Stat**\n",
    "\n",
    "first count characters need to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MapLang(code):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if code in LANG_CODES:\n",
    "        return LANG_CODES[code]\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "def count_clean_char(f):\n",
    "    df = pd.read_json(f, orient='records')\n",
    "    if df.size == 0:\n",
    "        print('empty: {}'.format(f))\n",
    "        return 0,0,None\n",
    "    foreign_df= df[df['fastText_lang']!='en']\n",
    "    foreign_df['clean_text'] = foreign_df['text'].map(lambda x:keepemoji_clean(x))\n",
    "    char_count = foreign_df['clean_text'].str.count('.')\n",
    "    return char_count.sum(),df.shape[0],df['fastText_lang']\n",
    "\n",
    "def calculateStat(count_path):\n",
    "    tweetfiles = glob.glob(count_path+'*.json')\n",
    "    total_char = 0.0\n",
    "    total_file = 0\n",
    "    total_records = 0\n",
    "    lang_col=[]\n",
    "    for f in tweetfiles:\n",
    "        m_count_char,m_records,m_lang_col = count_clean_char(f)\n",
    "        total_char += m_count_char\n",
    "        total_file += 1 if m_count_char > 0 else 0\n",
    "        total_records += m_records\n",
    "        if m_lang_col is not None:\n",
    "            lang_col.append(m_lang_col)\n",
    "        avg_char = total_char/total_file\n",
    "    lang_all = pd.concat(lang_col)\n",
    "    print(avg_char)\n",
    "    print(total_char)\n",
    "    print(total_records)\n",
    "    sns.countplot(lang_all)\n",
    "    count_table = pd.DataFrame(lang_all.value_counts()[:20])\n",
    "    count_table['lang_code'] = count_table.apply(lambda x:x.index)\n",
    "    count_table['lang_name'] = count_table['lang_code'].map(lambda x: MapLang(x))\n",
    "    count_table['portion'] = count_table['fastText_lang']/total_records\n",
    "    count_table['portion'].sum()\n",
    "    return count_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_key = AZURE_KEY\n",
    "endpoint = 'https://api-nam.cognitive.microsofttranslator.com/'\n",
    "path = '/translate?api-version=3.0'\n",
    "def translate_to_en(ori_text):\n",
    "    '''\n",
    "    Translate origin foreign text to English\n",
    "    Return: A tuple (translated text, detected origin language, score for language detection)\n",
    "    '''\n",
    "    if len(ori_text) == 0:\n",
    "        return ('','',0.0)\n",
    "    params = '&to=en'\n",
    "    constructed_url = endpoint + path + params\n",
    "    headers = {\n",
    "    'Ocp-Apim-Subscription-Key': subscription_key,\n",
    "    'Ocp-Apim-Subscription-Region':'eastus2',\n",
    "    'Content-type': 'application/json',\n",
    "    'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "    body = [{\n",
    "    'text': ori_text\n",
    "    }]\n",
    "    request = requests.post(constructed_url, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "    if response[0]:\n",
    "        return response[0]['translations'][0]['text'],\\\n",
    "    response[0]['detectedLanguage']['language'],\\\n",
    "    response[0]['detectedLanguage']['score']\n",
    "    else:\n",
    "        return ('','',0.0)\n",
    "#unit test\n",
    "#translate_to_en('你好世界！')\n",
    "\n",
    "def translate_df_on_lang(x):\n",
    "    '''\n",
    "    attach translated text, detected language, and confidence score to prigin dataframe\n",
    "    '''\n",
    "    if x['fastText_lang'] == 'en':\n",
    "        x['en_text'] = x['clean_text']\n",
    "        x['detect_lang'] = 'en'\n",
    "        x['google_trans'] = ''\n",
    "        x['lang_score'] = 1.0\n",
    "        \n",
    "    else:\n",
    "        res = translate_to_en(x['clean_text'])\n",
    "        x['en_text'] = res[0]\n",
    "        try:\n",
    "            x['google_trans'] = googleTrans(x['clean_text'])\n",
    "        except:\n",
    "            print('Error google trans: {}'.format(x['clean_text']))\n",
    "            x['google_trans'] = ''\n",
    "        x['detect_lang'] = res[1]\n",
    "        x['lang_score'] = res[2]\n",
    "    return x\n",
    "\n",
    "#caution: run this function will use api budget of Azure!!\n",
    "def run_translation(df):\n",
    "    df= df.apply(\n",
    "        translate_df_on_lang, \n",
    "        axis=1) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-d1f3f72cd25b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Object for Google translation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTRANSLATOR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgoogleTrans\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mtranslated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTRANSLATOR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Translator' is not defined"
     ]
    }
   ],
   "source": [
    "# Object for Google translation    \n",
    "TRANSLATOR = Translator()\n",
    "\n",
    "def googleTrans(text):\n",
    "    translated = TRANSLATOR.translate(text)\n",
    "    return translated.text\n",
    "#unit test\n",
    "# googleTrans('Gute Morgen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather term identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add Weather related term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckWeatherTerm(text):\n",
    "    '''\n",
    "    Return 1 or 0 for whether input contains any weather term\n",
    "    '''\n",
    "    words = nltk.word_tokenize(text)\n",
    "    for w in words:\n",
    "        if w in DICT_WEATHER_TERMS:\n",
    "            print(w)\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "#unit test\n",
    "#CheckWeatherTerm('A good weather!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Two: Compute Sentiment Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WKWSCI sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def loadWkwsciDict():\n",
    "    df = pd.read_csv(WKWSCI_PATH, sep='\\t')\n",
    "    wkwsci_dict = df.set_index(['term','POS'])['sentiment'].to_dict()\n",
    "    return wkwsci_dict\n",
    "wkwsci_dict = loadWkwsciDict()\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def WKWSCI_term_sentiment(wkwsci_dict,word,tag):\n",
    "    if tag == wn.ADJ:\n",
    "        pos = 'adj'\n",
    "    elif tag == wn.NOUN:\n",
    "        pos = 'n'\n",
    "    elif tag == wn.VERB:\n",
    "        pos = 'v'\n",
    "    elif tag == wn.ADV:\n",
    "        pos = 'adv'\n",
    "    if (word,pos) in wkwsci_dict:\n",
    "        return wkwsci_dict[(word,pos)]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "def WKWSCI_polarity(text,wkwsci_dict):\n",
    "    \"\"\"\n",
    "    Return a sentiment polarity\n",
    "    \"\"\"\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    " \n",
    "    #text = clean_text(text)\n",
    " \n",
    " \n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    for raw_sentence in raw_sentences:\n",
    "\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "     \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "\n",
    "            if not lemma:\n",
    "                continue\n",
    " \n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "\n",
    "            if not synsets:\n",
    "                continue\n",
    "            synset = synsets[0]\n",
    "            senti_word = synset.name().split('.')[0]\n",
    "            sentiment_score = WKWSCI_term_sentiment(wkwsci_dict,senti_word,wn_tag)\n",
    "             \n",
    "            sentiment += sentiment_score\n",
    "            tokens_count += 1\n",
    " \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    return 1.0*sentiment/tokens_count\n",
    "\n",
    "#unittest\n",
    "#WKWSCI_polarity('It is sad',wkwsci_dict)\n",
    "\n",
    "def add_wkwsci(df):\n",
    "    wkwsci_dict = loadWkwsciDict()\n",
    "    df['wkwsci'] = df[TEXT_FIELD].map(lambda x:WKWSCI_polarity(x,wkwsci_dict))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentiWordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def penn_to_wn(tag):\n",
    "    \"\"\"\n",
    "    Convert between the PennTreebank tags to simple Wordnet tags\n",
    "    \"\"\"\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def swn_polarity(text):\n",
    "    \"\"\"\n",
    "    Return a sentiment polarity: 0 = negative, 1 = positive\n",
    "    \"\"\"\n",
    " \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "\n",
    " \n",
    " \n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    for raw_sentence in raw_sentences:\n",
    "\n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "     \n",
    "        for word, tag in tagged_sentence:\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN, wn.ADJ, wn.ADV):\n",
    "                continue\n",
    " \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            #print('lemma: '+lemma)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
    "            #print('synsets: '+str(synsets))\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # Take the first sense, the most common\n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += swn_synset.pos_score() - swn_synset.neg_score()\n",
    "            tokens_count += 1\n",
    " \n",
    "    # judgment call ? Default to positive or negative\n",
    "    if not tokens_count:\n",
    "        return 0\n",
    " \n",
    "    # sum greater than 0 => positive sentiment\n",
    "    return sentiment\n",
    "#unit test\n",
    "#swn_polarity('Nice job!') # 1 1\n",
    "\n",
    "def add_swn(df):\n",
    "    df['swn'] = df[TEXT_FIELD].map(lambda x:swn_polarity(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texeblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_textblob(df):\n",
    "    df['textblob'] = df[TEXT_FIELD].map(lambda x:TextBlob(x).sentiment.polarity)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AFINN (Nielsen 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def afinn_sentiment(text,afinn,pattern_split):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence. \n",
    "    \"\"\"\n",
    "    words = pattern_split.split(text.lower())\n",
    "    sentiments = list(map(lambda word: afinn.get(word, 0), words))\n",
    "    leng = len(sentiments)\n",
    "    if leng > 0:\n",
    "        # How should you weight the individual word sentiments? \n",
    "        # You could do N, sqrt(N) or 1 for example. Here I use sqrt(N)\n",
    "        sentiment = float(sum(list(sentiments)))/math.sqrt(len(list(sentiments)))\n",
    "        \n",
    "    else:\n",
    "        sentiment = 0\n",
    "    return sentiment\n",
    "\n",
    "def add_afinn(df):\n",
    "    filenameAFINN = AFINN_PATH\n",
    "    afinn = dict(map(lambda ws: (ws[0], int(ws[1])), [ \n",
    "            ws.strip().split('\\t') for ws in open(filenameAFINN) ]))\n",
    "    pattern_split = re.compile(r\"\\W+\")\n",
    "    df['afinn'] = df[TEXT_FIELD].map(lambda x:afinn_sentiment(x,afinn,pattern_split))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hedonometer (Dodds and Danforth 2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_scores(filename):\n",
    "    \"\"\"Takes a file from the Dodd research paper and returns a dict of\n",
    "    wordscores. Note this function is tailored to the file provided\n",
    "    by the Dodd paper. For other sets of word scores, a dict can be\n",
    "    passed directly to HMeter.\"\"\"\n",
    "    \n",
    "    doddfile = csv.reader(open(filename, \"r\"), delimiter='\\t')\n",
    "    for x in range(4):  # strip header info\n",
    "        next(doddfile)\n",
    "\n",
    "    return {row[0]: float(row[2]) for row in doddfile}\n",
    "\n",
    "class HMeter(object):\n",
    "    \"\"\"HMeter is the main class to prepare a text sample for scores. It\n",
    "    expects a list of individual words, such as those provided by \n",
    "    nltk.word_tokenize, as wordlist. It expects a dict of words as k and\n",
    "    floating point wordscores as v for wordscores. deltah allows us to \n",
    "    filter out the most neutral words as stop words.\"\"\"\n",
    "\n",
    "    def __init__(self, wordlist, wordscores, deltah=0.0):\n",
    "        self.wordlist = wordlist\n",
    "        self.wordscores = wordscores\n",
    "        self.deltah = deltah\n",
    "\n",
    "    _deltah = None\n",
    "    @property\n",
    "    def deltah(self):\n",
    "        \"\"\"Deltah determines stop words. The higher deltah the more neutral \n",
    "        words are are discarded from the matchlist.\"\"\"\n",
    "        return self._deltah\n",
    "\n",
    "    @deltah.setter\n",
    "    def deltah(self, deltah):\n",
    "        \"\"\"Each time deltah is set we need to regenerate the matchlist.\"\"\"\n",
    "        self._deltah = deltah\n",
    "        # TODO Should probably raise a range error if deltah is nonsensical\n",
    "        # first we take every word that matches labMT 1.0\n",
    "        labmtmatches = (word for word in self.wordlist\n",
    "                        if word in self.wordscores)\n",
    "\n",
    "        # then we strip out stop words as described by Dodd paper\n",
    "        self.matchlist = []\n",
    "        for word in labmtmatches:\n",
    "            score = self.wordscores[word]\n",
    "            if score >= 5.0 + self.deltah or score <= 5.0 - self.deltah:\n",
    "                self.matchlist.append(word)\n",
    "\n",
    "    def fractional_abundance(self, word):\n",
    "        \"\"\"Takes a word and return its fractional abundance within\n",
    "        self.matchlist\"\"\"\n",
    "        frac_abund = self.matchlist.count(word) / len(self.matchlist)\n",
    "        return frac_abund\n",
    "\n",
    "    def word_shift(self, comp):\n",
    "        \"\"\"Produces data necessary to create a word shift graph. Returns a list \n",
    "        of tuples that contain each word's contribution to happiness score shift \n",
    "        between two samples. So for example, assigned to a variable 'output_data'\n",
    "        output_data[n] represents the data for one word where:\n",
    "            \n",
    "        output_data[n][0] the word\n",
    "        output_data[n][1] the proportional contribution the word gives to overall\n",
    "                          word shift\n",
    "        output_data[n][2] The relative abundance of word between the two samples\n",
    "        output_data[n][3] The word's happiness relative to the refernce sample\n",
    "        \n",
    "        Using this data, we can construct word shift graphs as described here:\n",
    "        http://www.hedonometer.org/shifts.html\"\"\"\n",
    "\n",
    "        # initialize variables for potentially large loop.\n",
    "        # create our comparison object. self is the reference object.\n",
    "        tcomp = HMeter(comp, self.deltah)\n",
    "\n",
    "        # we want a list of all potential words, but only need each word once.\n",
    "        word_shift_list = set(tcomp.matchlist + self.matchlist)\n",
    "\n",
    "        output_data = []\n",
    "        ref_happiness_score = self.happiness_score()\n",
    "        comp_happiness_score = tcomp.happiness_score()\n",
    "        happy_diff = comp_happiness_score - ref_happiness_score\n",
    "\n",
    "        for word in word_shift_list:\n",
    "            abundance = (tcomp.fractional_abundance(word) -\n",
    "                         self.fractional_abundance(word))\n",
    "            happiness_shift = self.wordscores[word] - ref_happiness_score\n",
    "            paper_score = (happiness_shift * abundance * 100) / happy_diff\n",
    "            output_data.append((word, paper_score, abundance, happiness_shift))\n",
    "\n",
    "        # sort words by absolute value of individual word shift\n",
    "        output_data.sort(key=lambda word: abs(word[1]))\n",
    "        return output_data\n",
    "\n",
    "    def happiness_score(self):\n",
    "        \"\"\"Takes a list made up of individual words and returns the happiness\n",
    "        score.\"\"\"\n",
    "\n",
    "        happysum = 0\n",
    "        count = len(self.matchlist)\n",
    "\n",
    "        for word in self.matchlist:\n",
    "            happysum += self.wordscores[word]\n",
    "\n",
    "        if count != 0:  # divide by zero errors are sad.\n",
    "            return happysum / count\n",
    "        else:\n",
    "            pass  # empty lists have no score\n",
    "        \n",
    "def hmeter_sentiment(text,pattern_split,scores):\n",
    "    \"\"\"\n",
    "    Returns a float for sentiment strength based on the input text.\n",
    "    Positive values are positive valence, negative value are negative valence. \n",
    "    \"\"\"\n",
    "    words = pattern_split.split(text.lower())\n",
    "    h = HMeter(words,scores)\n",
    "    return h.happiness_score()\n",
    "# unittest\n",
    "# hmeter_sentiment('VADER is not smart, handsome, nor funny')\n",
    "\n",
    "def add_hedono(df):\n",
    "    scores = load_scores(HEDONO_PATH)\n",
    "    pattern_split = re.compile(r\"\\W+\")\n",
    "    df['hedono'] = df[TEXT_FIELD].map(lambda x:hmeter_sentiment(x,pattern_split,scores))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VADER (Gilbert and Hutto 2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vader(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['vader'] = df[TEXT_FIELD].map(lambda x:analyzer.polarity_scores(x)['compound'])\n",
    "    return df\n",
    "#unittest\n",
    "# vs = analyzer.polarity_scores(\"VADER is VERY SMART, handsome, and FUNNY.\")\n",
    "# vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIWC(Pennebaker et al. 2015)\n",
    "\n",
    "Commercial software. To be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_polarity_weighted(df,weights = 1):\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_all_sentiment(df):\n",
    "    '''\n",
    "    calculate sentiment scores for field TEXT_FIELD\n",
    "    '''\n",
    "    df = add_afinn(df)\n",
    "    df = add_textblob(df)\n",
    "    df = add_hedono(df)\n",
    "    df = add_vader(df)\n",
    "    df = add_swn(df)\n",
    "    df = add_wkwsci(df)\n",
    "    df = add_polarity_weighted(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execuate All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_by_file(f):\n",
    "    df = pd.read_json(f, orient='records')\n",
    "    fn = f.split('/')[-1]\n",
    "    if df.size == 0:\n",
    "        print('empty: {}'.format(f))\n",
    "        return -1\n",
    "    df['lat'] = df['geo'].str['coordinates'].str[0]\n",
    "    df['lng'] = df['geo'].str['coordinates'].str[1]\n",
    "\n",
    "\n",
    "    df = df[['fastText_lang','id','text','tweet_created_at','lat','lng']]\n",
    "    df['clean_text'] = df['text'].map(lambda x:keepemoji_clean(x))\n",
    "    foreign_df= df[df['fastText_lang']!='en']\n",
    "    char_count = foreign_df['clean_text'].str.count('.')\n",
    "    df = run_translation(df)\n",
    "    if 'en_text' in df:\n",
    "        df['weather_term'] = df['en_text'].map(lambda x:CheckWeatherTerm(x))\n",
    "    else:\n",
    "        df['weather_term'] = df['clean_text'].map(lambda x:CheckWeatherTerm(x))\n",
    "    df = add_all_sentiment(df)\n",
    "    df = df.drop(columns=['text','clean_text'])\n",
    "    print(savepath+fn+'.csv')\n",
    "    df.to_csv(savepath+fn+'.csv',index=False)\n",
    "    return char_count.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate statistics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "count_path = READ_PATH #india tweets\n",
    "calculateStat(count_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = READ_PATH #india tweets\n",
    "savepath = SAVE_PATH\n",
    "\n",
    "tweetfiles = glob.glob(filepath+'*.json')\n",
    "\n",
    "used_char_count = 0\n",
    "file_processed = 0\n",
    "for f in tweetfiles[:\n",
    "    ret = process_all_by_file(f)\n",
    "    used_char_count+=ret if ret >= 0 else 0\n",
    "    file_processed += 1 if ret >= 0 else 0\n",
    "    print(used_char_count)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
